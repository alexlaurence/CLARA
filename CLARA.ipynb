{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLARA - Rat MRI Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "R3VV9yzHIyd-",
        "_GdOzYLzFVYb",
        "kZQy-lTNyFKr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlaurence/CLARA/blob/master/CLARA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TB8O5ymzyLIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CLARA (2019.0.2-alpha)\n",
        "\n",
        "*Convolutional Learning for Awake Rodent-MRI signal Analysis*\n",
        "\n",
        "[www.celestial.tokyo/~AlexLaurence/](http://www.celestial.tokyo/~AlexLaurence/)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4LPViTeFyMm5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is an AI-driven tool for predicting noise in awake-rodent fMRI for data-cleaning pipelines in preclinical research.\n",
        "\n",
        "* CLARA developed by [Alexander Laurence](mailto:alexander.adamlaurence@gmail.com) with the help of various open source neuroimaging tools (TensorFlow, Keras, nii2img, niwidgets).\n",
        "\n",
        "* RatNet developed by [Alexander Laurence](mailto:alexander.adamlaurence@gmail.com), trained on n=24 male Sprague Dawley® Rats fMRI images (~30,000 images in total). \n",
        "\n",
        "## Introduction\n",
        "\n",
        "### Relevance\n",
        "Rodent MRI is a powerful model for preclinical research exploring physiology, pathology and drug development. Particularly, awake rodent functional MRI can allow us to assess concious changes in brain activity across different mental states. However, without anaesthesia, head movement and long interscan intervals can contribute to occasional signal dropout. As such, this can lead to image quality inconsistency as shown in the reduction of grey value intensity. By detecting the images with reduced intensity (i.e. dark images), we aim to repair potential dropouts that may confound the blood oxygen level dependent signals. The goal of this tool is to develop an objective method of defining and detecting signal dropout as dark slices in the effort to repair affected volumes. \n",
        "\n",
        "### Training Data\n",
        "We scanned the brains of two groups of healthy male adult rats (n=24). The functional images were acquired by a 7-T scanner using a Fast Spin Echo sequence of a 64x64 matrix, TR=2500ms, ETL 16, and effective TE=36ms. \n",
        "\n",
        "Two experimenters inspected signal dropout at the coronal slices which could also be seen as dark bands in the axial and sagittal views. To provide an objective measurement, mean image intensity was calculated by using ImageJ. Subjective inspection from two experimenters shows highly consistent identification of dark images. Using the image intensity data, we plotted the distribution and established a threshold of detecting the dark images. We defined a threshold that best matching the subjective inspection and formed objective criteria that could be applied across datasets. \n",
        "\n",
        "### Objectives\n",
        "CLARA provides rodent fMRI pre-clinical studies with a pipeline for automatically detecting signal dropout in the form of dark slices. Through this approach, overall consistency of image quality may improve translational validity for researchers."
      ]
    },
    {
      "metadata": {
        "id": "5vSBWrXf6v9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 0. Building the Model"
      ]
    },
    {
      "metadata": {
        "id": "3G_R1o0A7k35",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.imgur.com/tAyfB72.png)\n",
        "\n",
        "\n",
        "The model (RatNet) is built using Keras, utilizing TensorFlow as the backend. TensorFlow was chosen as the backend due to better performance over Theano, and the ability to visualize the neural network using TensorBoard.\n",
        "\n",
        "For predicting two categories (good/bad), our model utilizes three convolutional layers, each having a depth of 32. A Max Pooling layer is applied after all three convolutional layers with size (2,2).\n",
        "\n",
        "After pooling, the data is fed through a single fully connected layer of size 128, and finally to the output layer, consisting of 2 softmax nodes.\n",
        "\n",
        "**Developers: Unfortunately I do not have permission from the lab to distribute the actual training data, or any raw files associated with it. Please use this as a guide towards building your own model for CLARA. As such RatNet remains closed-source. **"
      ]
    },
    {
      "metadata": {
        "id": "fByUfvmzF1BK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's begin by installing the Open source Computer Vision (OpenCV) library."
      ]
    },
    {
      "metadata": {
        "id": "OpygXp-JcB03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGf9QCs2b6ON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0.1 Shape Function\n",
        "We can get the shape of multi-dimensional Tensor:"
      ]
    },
    {
      "metadata": {
        "id": "GKqTiTw07j82",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.truncated_normal([16,128,128,3])\n",
        "sess = tf.Session()\n",
        "sess.run(tf.initialize_all_variables())\n",
        "sess.run(tf.shape(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLlMOyFycWBH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's reshape this to a new 2D Tensor of shape[16  128*128*3]= [16 49152]."
      ]
    },
    {
      "metadata": {
        "id": "l26fSydbcXRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b=tf.reshape(a,[16,49152])\n",
        "sess.run(tf.shape(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0YAA3R7c2iB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0.2 Reading Inputs\n",
        "\n",
        "The inputs are 30,000 images that I used for my MSc research project. Typically, we divide our input data into 3 parts:\n",
        "\n",
        "* **Training data**: we shall use 80% of the images for training.\n",
        "\n",
        "* **Validation data**: 20% images will be used for validation. These images are taken out of training data to calculate accuracy independently during the training process.\n",
        "\n",
        "* **Test set**: separate independent data for testing which has around 400 images. Sometimes due to something called Overfitting; after training, neural networks start working very well on the training data(and very similar images) i.e. the cost becomes very small, but they fail to work well for other images. For example, if you are training a classifier between dogs and cats and you get training data from someone who takes all images with white backgrounds. It’s possible that your network works very well on this validation data-set, but if you try to run it on an image with a cluttered background, it will most likely fail. So, that’s why we try to get our test-set from an independent source."
      ]
    },
    {
      "metadata": {
        "id": "7l7C4FuVc7Pw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = ['good', 'bad']\n",
        "num_classes = len(classes)\n",
        " \n",
        "train_path='training_data'\n",
        " \n",
        "# validation split\n",
        "validation_size = 0.2\n",
        " \n",
        "# batch size\n",
        "batch_size = 16\n",
        " \n",
        "data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oOK2F6ol_RBO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's upload our training data as one zip file..."
      ]
    },
    {
      "metadata": {
        "id": "qHqnMeM4_RNO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded_training = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded_training[fn])))\n",
        "  \n",
        "if not os.path.exists(\"/content/\" + train_path):\n",
        "        os.makedirs(\"/content/\" + train_path)\n",
        "    \n",
        "import shutil\n",
        "shutil.move(\"/content/\" + fn, \"/content/\" + train_path)\n",
        "\n",
        "!unzip \"/content/\" + train_path + \"/\" + fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ccBSjnmd67_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The objective of our training is to learn the correct values of weights/biases for all the neurons in the network that work to do classification between 'good' image and 'bad' image. Let’s create functions to create initial weights quickly just by specifying the shape"
      ]
    },
    {
      "metadata": {
        "id": "4HVr26eod7EH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_weights(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y88sad3neRvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_biases(size):\n",
        "    return tf.Variable(tf.constant(0.05, shape=[size]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJbQLcISeZTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0.3 Creating Network Layers\n",
        "\n",
        "**f.nn.conv2d** function can be used to build a convolutional layer which takes these inputs:\n",
        "\n",
        "* **input**= the output(activation) from the previous layer. This should be a 4-D tensor. Typically, in the first convolutional layer, you pass n images of size width*height*num_channels, then this has the size [n width height num_channels]\n",
        "\n",
        "* **filter**= trainable variables defining the filter. We start with a random normal distribution and learn these weights. It’s a 4D tensor whose specific shape is predefined as part of network design. If your filter is of size filter_size and input fed has num_input_channels and you have num_filters filters in your current layer, then filter will have following shape:\n",
        "\n",
        "\n",
        "\n",
        "> `[filter_size filter_size num_input_channels num_filters]`\n",
        "\n",
        "\n",
        "\n",
        "* **strides**= defines how much you move your filter when doing convolution. In this function, it needs to be a Tensor of size>=4 i.e. [batch_stride x_stride y_stride depth_stride]. batch_stride is always 1 as you don’t want to skip images in your batch. x_stride and y_stride are same mostly and the choice is part of network design and we shall use them as 1 in our example. depth_stride is always set as 1 as you don’t skip along the depth.\n",
        "\n",
        "* **padding**=SAME means we shall 0 pad the input such a way that output x,y dimensions are same as that of input.\n",
        "\n",
        "After convolution, we add the biases of that neuron, which are also learnable/trainable. Again we start with random normal distribution and learn these values during training.\n",
        "\n",
        "Now, we apply max-pooling using tf.nn.max_pool function that has a very similar signature as that of conv2d function."
      ]
    },
    {
      "metadata": {
        "id": "0KyK3CrgezLo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D76skrWEfCtT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we use a RELU as our activation function which simply takes the output of max_pool and applies RELU using **tf.nn.relu**.\n",
        "\n",
        "Here's our complete convolution layer:"
      ]
    },
    {
      "metadata": {
        "id": "HW_J506YfC1K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_convolutional_layer(input,\n",
        "               num_input_channels, \n",
        "               conv_filter_size,        \n",
        "               num_filters):  \n",
        "    \n",
        "    ## We shall define the weights that will be trained using create_weights function.\n",
        "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
        "    ## We create biases using the create_biases function. These are also trained.\n",
        "    biases = create_biases(num_filters)\n",
        " \n",
        "    ## Creating the convolutional layer\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                     filter=weights,\n",
        "                     strides=[1, 1, 1, 1],\n",
        "                     padding='SAME')\n",
        " \n",
        "    layer += biases\n",
        " \n",
        "    ## We shall be using max-pooling.  \n",
        "    layer = tf.nn.max_pool(value=layer,\n",
        "                            ksize=[1, 2, 2, 1],\n",
        "                            strides=[1, 2, 2, 1],\n",
        "                            padding='SAME')\n",
        "    ## Output of pooling is fed to Relu which is the activation function for us.\n",
        "    layer = tf.nn.relu(layer)\n",
        " \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vg5LMWQ-fXCC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Output of a convolutional layer is a multi-dimensional Tensor. We want to convert this into a one-dimensional tensor. This is done in the Flattening layer. We simply use the reshape operation to create a single dimensional tensor as defined below:"
      ]
    },
    {
      "metadata": {
        "id": "9T1aSntzfZ0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_flatten_layer(layer):\n",
        "    layer_shape = layer.get_shape()\n",
        "    num_features = layer_shape[1:4].num_elements()\n",
        "    layer = tf.reshape(layer, [-1, num_features])\n",
        " \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7M_pDj3fjpB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let’s define a function to create a fully connected layer. We do this by adding a condition that allows the caller to add RELU to the layer."
      ]
    },
    {
      "metadata": {
        "id": "3VNdESvPfd2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_fc_layer(input,          \n",
        "             num_inputs,    \n",
        "             num_outputs,\n",
        "             use_relu=True):\n",
        "    \n",
        "    #Let's define trainable weights and biases.\n",
        "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
        "    biases = create_biases(num_outputs)\n",
        " \n",
        "    layer = tf.matmul(input, weights) + biases\n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        " \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJGO1fiGgBGb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let’s create a placeholder that will hold the input training images. All the input images are read in dataset.py file and resized to 128 x 128 x 3 size. Input placeholder x is created in the shape of [None, 128, 128, 3]. The first dimension being None means you can pass any number of images to it. For this program, we shall pass images in the batch of 16 i.e. shape will be [16 128 128 3]. Similarly, we create a placeholder y_true for storing the predictions. For each image, we have two outputs i.e. probabilities for each class. Hence y_pred is of the shape [None 2] (for batch size 16 it will be [16 2]."
      ]
    },
    {
      "metadata": {
        "id": "ME2WR1q4gCBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels], name='x')\n",
        " \n",
        "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
        "y_true_cls = tf.argmax(y_true, dimension=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vt9qyZeYgMfl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use the functions defined above to create various layers of the network."
      ]
    },
    {
      "metadata": {
        "id": "r0ng24BxgM9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer_conv1 = create_convolutional_layer(input=x,\n",
        "               num_input_channels=num_channels,\n",
        "               conv_filter_size=filter_size_conv1,\n",
        "               num_filters=num_filters_conv1)\n",
        " \n",
        "layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
        "               num_input_channels=num_filters_conv1,\n",
        "               conv_filter_size=filter_size_conv2,\n",
        "               num_filters=num_filters_conv2)\n",
        " \n",
        "layer_conv3= create_convolutional_layer(input=layer_conv2,\n",
        "               num_input_channels=num_filters_conv2,\n",
        "               conv_filter_size=filter_size_conv3,\n",
        "               num_filters=num_filters_conv3)\n",
        "          \n",
        "layer_flat = create_flatten_layer(layer_conv3)\n",
        " \n",
        "layer_fc1 = create_fc_layer(input=layer_flat,\n",
        "                     num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
        "                     num_outputs=fc_layer_size,\n",
        "                     use_relu=True)\n",
        " \n",
        "layer_fc2 = create_fc_layer(input=layer_fc1,\n",
        "                     num_inputs=fc_layer_size,\n",
        "                     num_outputs=num_classes,\n",
        "                     use_relu=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VmHfxQngxvX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0.4 Predictions\n",
        "\n",
        "Now, let’s define the cost that will be minimized to reach the optimum value of weights. We will use a simple cost that will be calculated using a Tensorflow function softmax_cross_entropy_with_logits which takes the output of last fully connected layer and actual labels to calculate cross_entropy whose average will give us the cost."
      ]
    },
    {
      "metadata": {
        "id": "Zznr55Z_g00m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)\n",
        "cost = tf.reduce_mean(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MX_we68qhAFZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0.5 Optimisation\n",
        "\n",
        "Tensorflow implements most of the optimisation functions. We shall use AdamOptimizer for gradient calculation and weight optimization. We shall specify that we are trying to minimise cost with a learning rate of 0.0001."
      ]
    },
    {
      "metadata": {
        "id": "XBWyZASXhd50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hyiL-_eBhAOS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        " \n",
        "x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
        " \n",
        "feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
        " \n",
        "session.run(optimizer, feed_dict=feed_dict_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5cAZhBbhpPu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "where next_batch is a simple python function in dataset.py file that returns the next 16 images to be passed for training. Similarly, we pass the validation batch of images independently to in another session.run() call."
      ]
    },
    {
      "metadata": {
        "id": "sB8zq9wkhpyP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
        " \n",
        "feed_dict_val = {x: x_valid_batch, y_true: y_valid_batch}\n",
        " \n",
        "val_loss = session.run(cost, feed_dict=feed_dict_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AsVsjOPViE1L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can calculate the accuracy on validataion set using true labels(y_true) and predicted labels(y_pred)."
      ]
    },
    {
      "metadata": {
        "id": "HEKL3Icbh_94",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSInTBbriK43",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can calculate the validation accuracy by passing accuracy in session.run() and providing validation images in a feed_dict."
      ]
    },
    {
      "metadata": {
        "id": "hRCl_TzaiLmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_acc = session.run(accuracy,feed_dict=feed_dict_validate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMKt7-ndiPSZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Similarly, we also report the accuracy for the training images."
      ]
    },
    {
      "metadata": {
        "id": "O4DRstwpiUq3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc = session.run(accuracy, feed_dict=feed_dict_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mtcGjM7Sih7b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As, training images along with labels are used for training, so in general training accuracy will be higher than validation. We report training accuracy to know that we are at least moving in the right direction and are at least improving accuracy in the training dataset. After each Epoch, we report the accuracy numbers and save the model using saver object in Tensorflow."
      ]
    },
    {
      "metadata": {
        "id": "6hISv_qdiiut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver.save(session, 'RatNet-model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L43gE8Vbiz_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's the complete train function"
      ]
    },
    {
      "metadata": {
        "id": "0teYs0J0i1S0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(num_iteration):\n",
        "    global total_iterations\n",
        "    \n",
        "    for i in range(total_iterations,\n",
        "                   total_iterations + num_iteration):\n",
        " \n",
        "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
        "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(batch_size)\n",
        " \n",
        "        \n",
        "        feed_dict_tr = {x: x_batch,\n",
        "                           y_true: y_true_batch}\n",
        "        feed_dict_val = {x: x_valid_batch,\n",
        "                              y_true: y_valid_batch}\n",
        " \n",
        "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
        " \n",
        "        if i % int(data.train.num_examples/batch_size) == 0: \n",
        "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
        "            epoch = int(i / int(data.train.num_examples/batch_size))    \n",
        "            \n",
        "            show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)\n",
        "            saver.save(session, 'RatNet-model') \n",
        " \n",
        " \n",
        "    total_iterations += num_iteration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVmqqzbCFFpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Input Data"
      ]
    },
    {
      "metadata": {
        "id": "XnIsMnw-Gb09",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Imports"
      ]
    },
    {
      "metadata": {
        "id": "zWZ8FNrgp1zJ",
        "colab_type": "code",
        "outputId": "ff150c16-4df4-4a62-b8f8-1fcb679af572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1516
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install niwidgets\n",
        "!pip install nibabel nilearn\n",
        "!pip install shutil"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting niwidgets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/0b/d9001238ce9997ea970afc80d512a511b07413327034f565807acdaafc2e/niwidgets-0.2.1.tar.gz (8.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.2MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from niwidgets) (7.4.2)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (from niwidgets) (2.3.3)\n",
            "Collecting ipyvolume (from niwidgets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/e4/4a6754d2c390ee627f82aeaae76b34a1daf30eea80af41215eebedb0f5ce/ipyvolume-0.5.1-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from niwidgets) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from niwidgets) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from niwidgets) (1.1.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->niwidgets) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->niwidgets) (3.4.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->niwidgets) (4.4.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->niwidgets) (4.6.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->niwidgets) (4.3.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel->niwidgets) (1.11.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel->niwidgets) (0.98)\n",
            "Collecting pythreejs>=1.0.0 (from ipyvolume->niwidgets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/0e/f6dbe6e1a9a489cca1bcada3ee0ae5fd49cfb123c6829cc9c1c6583a3fa8/pythreejs-2.0.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.2MB 5.9MB/s \n",
            "\u001b[?25hCollecting traittypes (from ipyvolume->niwidgets)\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/d1/8d5bd662703cc1764d986f6908a608777305946fa634d34c470cd4a1e729/traittypes-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from ipyvolume->niwidgets) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ipyvolume->niwidgets) (2.18.4)\n",
            "Collecting ipywebrtc (from ipyvolume->niwidgets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/65/68b09fe23911f8c50914646253103fa9e6dc72cb2f779169c278c0818fc4/ipywebrtc-0.4.3.tar.gz (530kB)\n",
            "\u001b[K    100% |████████████████████████████████| 532kB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->niwidgets) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->niwidgets) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->niwidgets) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->niwidgets) (2.3.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (4.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (40.9.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (1.0.15)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (4.4.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (5.2.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->niwidgets) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->niwidgets) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->niwidgets) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets->niwidgets) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets->niwidgets) (5.2.4)\n",
            "Collecting ipydatawidgets>=1.1.1 (from pythreejs>=1.0.0->ipyvolume->niwidgets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/9e/16c3cbc63862b36c0b18aa0e1d1dd6a65496d4f8a91a57a728d19bdb74b1/ipydatawidgets-4.0.0-py2.py3-none-any.whl (250kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->ipyvolume->niwidgets) (0.46)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ipyvolume->niwidgets) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ipyvolume->niwidgets) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ipyvolume->niwidgets) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ipyvolume->niwidgets) (3.0.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->niwidgets) (0.1.7)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (5.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (2.10)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (0.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->niwidgets) (17.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (1.4.2)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (0.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (0.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets->niwidgets) (0.5.1)\n",
            "Building wheels for collected packages: niwidgets, ipywebrtc\n",
            "  Building wheel for niwidgets (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/72/21/a0/023d49343789e831207a7e7050086a20ff6400e807b30282ac\n",
            "  Building wheel for ipywebrtc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7a/22/84/e51d1d5fff13256af8ba0f246ba2fd1196f631d32306ac2f63\n",
            "Successfully built niwidgets ipywebrtc\n",
            "Installing collected packages: traittypes, ipydatawidgets, pythreejs, ipywebrtc, ipyvolume, niwidgets\n",
            "Successfully installed ipydatawidgets-4.0.0 ipyvolume-0.5.1 ipywebrtc-0.4.3 niwidgets-0.2.1 pythreejs-2.0.2 traittypes-0.2.1\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (2.3.3)\n",
            "Collecting nilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/9e/96f2da387ee9acaba2cbab7b596486b0231b5f6d9bf946b880536d4485fc/nilearn-0.5.0-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel) (0.98)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.14.6)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.5.0\n",
            "Collecting shutil\n",
            "\u001b[31m  Could not find a version that satisfies the requirement shutil (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for shutil\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zXRmkzaNpfU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Select NIfTI file"
      ]
    },
    {
      "metadata": {
        "id": "aWSb9mhkpnHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Select a single NIfTI (.nii) image. Use the widget to verify your image. \n",
        "\n",
        "Nb. Widgets are not fully operational on Google Colab at the moment, but you can run these scripts elsewhere if needed."
      ]
    },
    {
      "metadata": {
        "id": "ZIH6WNZ_qObh",
        "colab_type": "code",
        "outputId": "55c0534a-a496-429b-a291-b481fae0842f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "#from niwidgets import NiftiWidget\n",
        "#import nilearn.plotting as nip\n",
        "#print('Plotting ' + fn)\n",
        "#my_widget = NiftiWidget('/content/' + fn)\n",
        "#my_widget.nifti_plotter(plotting_func=nip.plot_img, display_mode=['ortho', 'x', 'y', 'z'], colormap='gray')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-84611ec4-17e0-4184-bd53-02e3cc12adbd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-84611ec4-17e0-4184-bd53-02e3cc12adbd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving f105x1.nii to f105x1.nii\n",
            "User uploaded file \"f105x1.nii\" with length 246112 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PAyIdStY3S6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from niwidgets import NiftiWidget\n",
        "import nilearn.plotting as nip\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  my_widget = NiftiWidget('/content/' + fn)\n",
        "  my_widget.nifti_plotter(plotting_func=nip.plot_img, display_mode=['ortho', 'x', 'y', 'z'], colormap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQNOk7v8immO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.3 Convert NIfTI to Image \n",
        "\n",
        "Conversion tool developed from **nii2img** (https://github.com/medihack/nii2img).\n",
        "\n",
        "Copyright © 2006-2013 Jessica Jesser and Kai Schlamp. All rights reserved. "
      ]
    },
    {
      "metadata": {
        "id": "l_2dZlMUNyJU",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Basic Header Information\n",
        "slices = 15 #@param {type:\"number\"}\n",
        "volumes = 1\n",
        "totalimg = slices*volumes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-4osDt8YK5A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "Y12hjZ7jI6ny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import getopt\n",
        "import nibabel as nib\n",
        "import numpy\n",
        "import scipy\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-cdFVovYOQV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Parameters"
      ]
    },
    {
      "metadata": {
        "id": "mWo4ibNgJtmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Nii\n",
        "\n",
        "Load NIfTI file according to parameters above"
      ]
    },
    {
      "metadata": {
        "id": "SdaGXRWsJaou",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Enter NIfTI Data Paramters\n",
        "\n",
        "# params is the dictionary holding all needed parameters for the script\n",
        "params = {\n",
        "    'nii_filename': \"\",\n",
        "    'start_slice': 0,\n",
        "    'end_slice': None,\n",
        "    'alternate_slice': 1,\n",
        "    'plane': \"ax\",\n",
        "    'output_prefix': \"img\",\n",
        "    'format': \"png\",\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "first_slice = 0 #@param {type:\"number\"}\n",
        "alt_slice = 1 #@param {type:\"number\"}\n",
        "plane_view = \"ax\" #@param {type:\"string\"}\n",
        "prefix_filename = \"img\" #@param {type:\"string\"}\n",
        "file_format = \"png\" #@param {type:\"string\"}\n",
        "output_folder = \"/preprocessed/\" #@param {type:\"string\"}\n",
        "\n",
        "dst = \"/content\" + output_folder\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "        os.makedirs(dst)\n",
        "\n",
        "def load_nii(params):\n",
        "    if not params['nii_filename']:\n",
        "        print(\"Error! Missing NIFTI input file.\")\n",
        "        sys.exit(2)\n",
        "\n",
        "    if params['verbose']:\n",
        "        print(\"Loading NIfTI file\")\n",
        "\n",
        "    img = nib.load(params['nii_filename'])\n",
        "    data = img.get_data()\n",
        "    return data\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  data = load_nii(params ={\n",
        "      'nii_filename': '/content/' + fn, \n",
        "      'start_slice': first_slice, \n",
        "      'end_slice': None, \n",
        "      'alternate_slice': alt_slice, \n",
        "      'plane': plane_view, \n",
        "      'output_prefix': prefix_filename, \n",
        "      'format': file_format, \n",
        "      'verbose': False\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7dZiebYJwyL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalise Nii\n",
        "\n",
        "Normalize the color value range in the original nii image"
      ]
    },
    {
      "metadata": {
        "id": "zWjOI10gJxAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_nii(params, data):\n",
        "    if params['verbose']:\n",
        "        print(\"Normalizing intensities\")\n",
        "\n",
        "normalize_nii(params, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pr7nxX5zKyLE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract Nii\n",
        "\n",
        "Extract the images from the file you wish to continue with, alternation, start and end slice"
      ]
    },
    {
      "metadata": {
        "id": "PcDqG2AYKtGT",
        "colab_type": "code",
        "outputId": "b470875e-ded5-43db-ffa6-136a3dd502f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "def extract_img(params, data):\n",
        "    if params['verbose']:\n",
        "        print(\"Extracting images\")\n",
        "        \n",
        "    for fn in uploaded.keys():\n",
        "      print('Filename: ' + fn)\n",
        "\n",
        "    image_counter = 0\n",
        "    slice_counter = 0\n",
        "    start_slice = params['start_slice'] or 0\n",
        "    start_slice = start_slice - 1 if start_slice > 0 else start_slice\n",
        "    end_slice = params['end_slice'] or data.shape[2]\n",
        "    for i in range(start_slice, end_slice):\n",
        "        if (slice_counter % params['alternate_slice']) == 0:\n",
        "            if params['plane'] == \"ax\":\n",
        "                slice = data[:, :, i]\n",
        "            elif params['plane'] == \"cor\":\n",
        "                slice = data[:, i, :]\n",
        "            elif params['plane'] == \"sag\":\n",
        "                slice = data[i, :, :]\n",
        "\n",
        "            slice = numpy.rot90(slice)\n",
        "\n",
        "            image_name = params['output_prefix'] + \"-\"+ fn[:-4] + \"-\" \"{:0>2}\".format(str(image_counter)) + \".\" + params['format']\n",
        "            scipy.misc.imsave(image_name, slice)\n",
        "            image_counter += 1\n",
        "            if params['verbose']:\n",
        "                sys.stdout.write(\".\")\n",
        "                sys.stdout.flush()\n",
        "            \n",
        "        src = \"/content/\" + image_name\n",
        "        shutil.move(src, dst)\n",
        "        slice_counter += 1\n",
        "\n",
        "    if params['verbose']:\n",
        "        print(\"\")\n",
        "        \n",
        "extract_img(params, data)\n",
        "\n",
        "t = 0\n",
        "plt.figure(figsize=(10, 50))\n",
        "for n in range(totalimg):\n",
        "  plt.subplot(18,5,n+1)\n",
        "  plt.imshow(data[n])\n",
        "  plt.title('%i' %t)\n",
        "  t = t +1\n",
        "  plt.axis('off')\n",
        "#plt.suptitle(\"Awake Rodent MRI Slices\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename: f105x1.nii\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:217: DeprecationWarning: `toimage` is deprecated!\n",
            "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use Pillow's ``Image.fromarray`` directly instead.\n",
            "  im = toimage(arr, channel_axis=2)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:337: DeprecationWarning: `bytescale` is deprecated!\n",
            "`bytescale` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "  cmin=cmin, cmax=cmax)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yEU24vDj-YOG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "plt.figure(figsize=(10, 50))\n",
        "\n",
        "for n in range(totalimg):\n",
        "  plt.subplot(18,5,n+1)\n",
        "  plt.imshow(data[n])\n",
        "  plt.title('%i' %t)\n",
        "  t = t +1\n",
        "  plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3VV9yzHIyd-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.4 Crop and Resize Image (WIP)"
      ]
    },
    {
      "metadata": {
        "id": "zW1aWDAQI9fH",
        "colab_type": "code",
        "outputId": "94c70b91-fd3f-457c-e8ff-9ec24089ffc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3I4bLVjUI62P",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Crop/Resize Image\n",
        "x_crop = 1 #@param {type:\"number\"}\n",
        "y_crop = 1 #@param {type:\"number\"}\n",
        "newsize = 109 #@param {type:\"number\"}\n",
        "\n",
        "def create_directory(directory):\n",
        "    '''\n",
        "    Creates a new folder in the specified directory if the folder doesn't exist.\n",
        "    INPUT\n",
        "        directory: Folder to be created, called as \"folder/\".\n",
        "    OUTPUT\n",
        "        New folder in the current directory.\n",
        "    '''\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def crop_and_resize_images(path, new_path, cropx, cropy, img_size):\n",
        "    '''\n",
        "    Crops, resizes, and stores all images from a directory in a new directory.\n",
        "    INPUT\n",
        "        path: Path where the current, unscaled images are contained.\n",
        "        new_path: Path to save the resized images.\n",
        "        img_size: New size for the rescaled images.\n",
        "    OUTPUT\n",
        "        All images cropped, resized, and saved from the old folder to the new folder.\n",
        "    '''\n",
        "    create_directory(new_path)\n",
        "    dirs = [l for l in os.listdir(path) if l != '.DS_Store']\n",
        "    total = 0\n",
        "\n",
        "    for item in dirs:\n",
        "        img = io.imread(path+item)\n",
        "        y,x = img.shape\n",
        "        startx = x//2-(cropx//2)\n",
        "        starty = y//2-(cropy//2)\n",
        "        img = img[starty:starty+cropy,startx:startx+cropx]\n",
        "        img = resize(img, (newsize,newsize))\n",
        "        io.imsave(str(new_path + item), img)\n",
        "        #src = \"/content/preprocessed/\" + item\n",
        "        #dst = \"/content/preprocessed/cr\"\n",
        "        #shutil.move(src, dst)\n",
        "        total += 1\n",
        "        print(\"Cropped/Resized: \", item, total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xOX0_lnJDuQ",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "39830566-c5b8-423d-c841-0ac082aace03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "crop_and_resize_images(path='/content/preprocessed/', new_path='/content/cropped/', cropx=x_crop, cropy=y_crop, img_size=newsize)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cropped/Resized:  img-f105x1-02.png 1\n",
            "Cropped/Resized:  img-f105x1-07.png 2\n",
            "Cropped/Resized:  img-f105x1-10.png 3\n",
            "Cropped/Resized:  img-f105x1-08.png 4\n",
            "Cropped/Resized:  img-f105x1-14.png 5\n",
            "Cropped/Resized:  img-f105x1-00.png 6\n",
            "Cropped/Resized:  img-f105x1-09.png 7\n",
            "Cropped/Resized:  img-f105x1-05.png 8\n",
            "Cropped/Resized:  img-f105x1-06.png 9\n",
            "Cropped/Resized:  img-f105x1-04.png 10\n",
            "Cropped/Resized:  img-f105x1-13.png 11\n",
            "Cropped/Resized:  img-f105x1-03.png 12\n",
            "Cropped/Resized:  img-f105x1-11.png 13\n",
            "Cropped/Resized:  img-f105x1-01.png 14\n",
            "Cropped/Resized:  img-f105x1-12.png 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-02.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-07.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-10.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-08.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-14.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-00.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-09.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-05.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-06.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-04.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-13.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-03.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-11.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-01.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py:132: UserWarning: /content/preprocessed/img-f105x1-12.png is a low contrast image\n",
            "  warn('%s is a low contrast image' % fname)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C5F0oEn44HBl",
        "colab_type": "code",
        "outputId": "83ff605a-021e-4992-faca-ca498db3da17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "img = io.imread('/content/preprocessed/img-f105x1-00.png')\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_GdOzYLzFVYb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5 Convert Image to Array (WIP)"
      ]
    },
    {
      "metadata": {
        "id": "kBy9rjtzEL2i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converts each image to an array, and appends each array to a new NumPy array, based on the image column equaling the image file name."
      ]
    },
    {
      "metadata": {
        "id": "pSwZJ_ti61Hq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def change_image_name(df, column):\n",
        "    \"\"\"\n",
        "    Appends the suffix '.jpeg' for all image names in the DataFrame\n",
        "    INPUT\n",
        "        df: Pandas DataFrame, including columns to be altered.\n",
        "        column: The column that will be changed. Takes a string input.\n",
        "    OUTPUT\n",
        "        Pandas DataFrame, with a single column changed to include the\n",
        "        aforementioned suffix.\n",
        "    \"\"\"\n",
        "    return [i + '.jpeg' for i in df[column]]\n",
        "\n",
        "\n",
        "def convert_images_to_arrays_train(file_path, df):\n",
        "    \"\"\"\n",
        "    Converts each image to an array, and appends each array to a new NumPy\n",
        "    array, based on the image column equaling the image file name.\n",
        "    INPUT\n",
        "        file_path: Specified file path for resized test and train images.\n",
        "        df: Pandas DataFrame being used to assist file imports.\n",
        "    OUTPUT\n",
        "        NumPy array of image arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    lst_imgs = [l for l in df['train_image_name']]\n",
        "\n",
        "    return np.array([np.array(Image.open(file_path + img)) for img in lst_imgs])\n",
        "\n",
        "\n",
        "def save_to_array(arr_name, arr_object):\n",
        "    \"\"\"\n",
        "    Saves data object as a NumPy file. Used for saving train and test arrays.\n",
        "    INPUT\n",
        "        arr_name: The name of the file you want to save.\n",
        "            This input takes a directory string.\n",
        "        arr_object: NumPy array of arrays. This object is saved as a NumPy file.\n",
        "    OUTPUT\n",
        "        NumPy array of image arrays\n",
        "    \"\"\"\n",
        "    return np.save(arr_name, arr_object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X7RPwt8lEVq6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "labels = pd.read_csv(\"/content/trainLabels.csv\")\n",
        "\n",
        "print(\"Writing Train Array\")\n",
        "X_train = convert_images_to_arrays_train('/content/preprocessed/', labels)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "print(\"Saving Train Array\")\n",
        "save_to_array('/content/X_train.npy', X_train)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZQy-lTNyFKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Predict Noise"
      ]
    },
    {
      "metadata": {
        "id": "FlaIiT6auaxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os,glob,cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXYc3iRz0JhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load a single image to try the model on.\n",
        "\n",
        "File RatNet-model.meta contains the complete network graph and we can use this to recreate the graph later. We shall use a saver object provided by Tensorflow to do this."
      ]
    },
    {
      "metadata": {
        "id": "oKBbW8IkyILs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.import_meta_graph('RatNet-model.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yo9ojKm7p1lS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The file RatNet-model.data-00000-of-00001 contains the trained weights(values of variables) of the network. So, once we have recreated the graph, we shall restore the weights."
      ]
    },
    {
      "metadata": {
        "id": "WpxSWP76zsln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver.restore(sess, tf.train.latest_checkpoint('./'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZyeYdCmX0FXf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to get the prediction of the network, we need to read & pre-process the input image in the same way(as training), get hold of y_pred on the graph and pass it the new image in a feed dict. So, let’s do that:"
      ]
    },
    {
      "metadata": {
        "id": "ItkgLS8wvb84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(filename)\n",
        "\n",
        "# Resizing the image to our desired size and preprocessing will be done exactly as done during training\n",
        "image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "images.append(image)\n",
        "images = np.array(images, dtype=np.uint8)\n",
        "images = images.astype('float32')\n",
        "images = np.multiply(images, 1.0/255.0) \n",
        "\n",
        "#The input to the network is of shape [None image_size image_size num_channels]. Hence we reshape.\n",
        "x_batch = images.reshape(1, image_size,image_size,num_channels)\n",
        " \n",
        "graph = tf.get_default_graph()\n",
        " \n",
        "y_pred = graph.get_tensor_by_name(\"y_pred:0\")\n",
        " \n",
        "## Let's feed the images to the input placeholders\n",
        "x= graph.get_tensor_by_name(\"x:0\") \n",
        "y_true = graph.get_tensor_by_name(\"y_true:0\") \n",
        "y_test_images = np.zeros((1, 2)) \n",
        " \n",
        "feed_dict_testing = {x: x_batch, y_true: y_test_images}\n",
        "result=sess.run(y_pred, feed_dict=feed_dict_testing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJ-TSJY8z8XS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we can run a new image of good/bad using predict script."
      ]
    },
    {
      "metadata": {
        "id": "Fk167PVoz9YI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Input Image Directory\n",
        "# First, pass the path of the image\n",
        "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
        "image_path=\"\" #@param {type:\"string\"}\n",
        "filename = dir_path +'/' +image_path\n",
        "image_size=128\n",
        "num_channels=3\n",
        "images = []\n",
        "\n",
        "# Reading the image using OpenCV\n",
        "image = cv2.imread(filename)\n",
        "\n",
        "# Resizing the image to our desired size and preprocessing will be done exactly as done during training\n",
        "image = cv2.resize(image, (image_size, image_size),0,0, cv2.INTER_LINEAR)\n",
        "images.append(image)\n",
        "images = np.array(images, dtype=np.uint8)\n",
        "images = images.astype('float32')\n",
        "images = np.multiply(images, 1.0/255.0) \n",
        "\n",
        "#The input to the network is of shape [None image_size image_size num_channels]. Hence we reshape.\n",
        "x_batch = images.reshape(1, image_size,image_size,num_channels)\n",
        "\n",
        "## Let us restore the saved model \n",
        "sess = tf.Session()\n",
        "\n",
        "# Step-1: Recreate the network graph. At this step only graph is created.\n",
        "saver = tf.train.import_meta_graph('RatNet-model.meta')\n",
        "\n",
        "# Step-2: Now let's load the weights saved using the restore method.\n",
        "saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
        "\n",
        "# Accessing the default graph which we have restored\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "# Now, let's get hold of the op that we can be processed to get the output.\n",
        "# In the original network y_pred is the tensor that is the prediction of the network\n",
        "y_pred = graph.get_tensor_by_name(\"y_pred:0\")\n",
        "\n",
        "## Let's feed the images to the input placeholders\n",
        "x= graph.get_tensor_by_name(\"x:0\") \n",
        "y_true = graph.get_tensor_by_name(\"y_true:0\") \n",
        "y_test_images = np.zeros((1, len(os.listdir('training_data')))) \n",
        "\n",
        "\n",
        "### Creating the feed_dict that is required to be fed to calculate y_pred \n",
        "feed_dict_testing = {x: x_batch, y_true: y_test_images}\n",
        "result=sess.run(y_pred, feed_dict=feed_dict_testing)\n",
        "\n",
        "# result is of this format [probabiliy_of_rose probability_of_sunflower]\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3V7Pq3TsOc0P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Future Work\n",
        "\n",
        "* 4D time-series fMRI data support. Currenly only supports 3D (or 4D-segmented data).\n",
        "* Program the neural network to retrain with new photos. This is a common practice, and only serves to optimize the model. Checks would be put in place to validate the images before being added to the classifier, in order to prevent low quality images from altering the classifier too drastically.\n",
        "\n",
        "* Port the Keras model to CoreML, and deploy to an RatNet iOS application. CoreML is a framework designed by Apple for adding machine learning to iOS devices. This allows the ability of Python developers to export their models, convert the file to a .mlmodel file, and add the file to the iOS development cycle.\n",
        "\n",
        "# 4. Acknowledgement\n",
        "\n",
        "For my initial research in which this tool is based on, I acknowledge the Royal Society (grant) and the Wellcome Trust (ISSF) for funding support, as well as Dr. Cyril Pernet, Dr. Gerry Thompson, Dr. Maurits Jansen, and Dr. Anjanette Harris of the University of Edinburgh for technical advice. I also extend my gratitude to our colleagues at the Laboratory of Magnetic Resonance Research (Chang-Gung University, Taiwan), Sun-Lin Han, Dr. Jiun-Jie Wang, and Dr. Chih-Chien Tsai for collaborative research exchange that helped to build the foundations for the 2nd part of my MSc thesis. Finally, I would like to thank my supervisor Dr. Szu-Han Wang at the Centre of Clinical Brain Sciences (CCBS) who's unwavering patience, perseverance, and knowledge has supported me throughout my project.\n",
        "\n",
        "In order to build CLARA, I made use of Ankit Sachan's helpful TensorFlow [tutorials](https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/) and [scripts](https://github.com/sankit1/cv-tricks.com/tree/master/Tensorflow-tutorials/tutorial-2-image-classifier) for image classification. I thank NIPY developers for [Neuro Widgets](https://github.com/nipy/niwidgets). I also made use of [nii2img](https://github.com/medihack/nii2img) for the conversion scripts, of which I thank Jessica Jesser and Kai Schlamp for developing. Furthermore, I also would like to extend my thanks to Siraj Raval for the inspiring deep learning in medical-imaging [guides](https://github.com/llSourcell/AI_in_Medicine_Clinical_Imaging_Classification#neural-network-architecture). \n",
        "\n",
        "Finally, I want to thank OpenCV and everyone at Google (especially at TensorFlow) for providing the tools to make all this possible."
      ]
    }
  ]
}